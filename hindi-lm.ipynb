{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# threshold for minimum count to be considered a valid word\n",
    "MIN_VOCAB_COUNT = 3\n",
    "OOV_TOKEN = \"UNK\"\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='r', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# remove numbers. they can be like 100 1090,200 2.123 etc\n",
    "# strategy is to remove punctuation and then check if its an integer\n",
    "def isNumber(word):\n",
    "    word_no_num = re.sub(r'[^\\w\\s]','',word)\n",
    "    if RepresentsInt(word_no_num):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#tokenizes raw strings\n",
    "def getTokenized(lines):\n",
    "    exclude = set(string.punctuation)\n",
    "    exclude.add('-')\n",
    "    exclude.add('।')\n",
    "    words_list = [] \n",
    "    total_sent = len(lines)\n",
    "    n = 0\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        words = nltk.word_tokenize(line)\n",
    "        words_nopunc_nonum = []\n",
    "        for word in words:\n",
    "            if word in exclude: # if punctuation\n",
    "                continue\n",
    "            else:\n",
    "                word = word.replace('।', '')\n",
    "                if(isNumber(word)): # if number\n",
    "                    word = \"NUMBER\"\n",
    "                words_nopunc_nonum.append(word)\n",
    "#         if(len(words_nopunc_nonum) >= 1):\n",
    "#             words_nopunc_nonum[-1] = words_nopunc_nonum[-1].replace('।', '')\n",
    "        words_list.append(words_nopunc_nonum)\n",
    "        if(n%10000 == 0):\n",
    "            print((n/total_sent)*100.0, '% done')\n",
    "        n+= 1\n",
    "    return words_list\n",
    "\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "def getVocabulary(tokenized_corpus):\n",
    "    vocabulary = {}\n",
    "    total_sent = len(tokenized_corpus)\n",
    "    n = 0\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = 1\n",
    "            else:\n",
    "                vocabulary[token] += 1\n",
    "        if(n%10000 == 0):\n",
    "            print((n/total_sent)*100.0, '% done')\n",
    "        n+= 1\n",
    "    new_dict = {}\n",
    "    oov_count = 0 \n",
    "    # remove infrequent words\n",
    "    for word, count in vocabulary.items():\n",
    "        if(count >= MIN_VOCAB_COUNT):\n",
    "            new_dict[word] = count\n",
    "        else:\n",
    "            oov_count += count\n",
    "    new_dict[OOV_TOKEN] = oov_count\n",
    "    word2id = {w: idx for (idx, w) in enumerate(new_dict)}\n",
    "    id2word = {idx: w for (idx, w) in enumerate(new_dict)}\n",
    "    return new_dict, word2id, id2word\n",
    "\n",
    "def removeOOV(sentences, vocab):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sent = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                new_sent.append(word)\n",
    "            else:\n",
    "                new_sent.append(OOV_TOKEN)\n",
    "        new_sentences.append(new_sent)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi = load_doc('data/hindi/hindmonocorp05.plaintext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4359377142"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hwt2013\\t<s>\\tलेकिन गांव के जगदीश मेघवाल, मोहन...\\nspiderling\\t<s>\\tविटामिन सी शरीर में रोग पैदा करने वाल'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi = hindi[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = hindi.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44486483"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['लेकिन गांव के जगदीश मेघवाल, मोहन...',\n",
       " 'विटामिन सी शरीर में रोग पैदा करने वाले विषाणुओं से लड़ने की ताकत पैदा करता है और शरीर में इसकी संतुलित मात्रा बने रहने से रोग प्रतिरोधक क्षमता मजबूत रहती है।',\n",
       " 'इन बोतलों के बहुत कम पैसे मिलते हैं।',\n",
       " 'कार्टून :- रे लोकपाल आ गया तू ? शाबाश.... 19 0']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sentences = []\n",
    "for line in sentences:\n",
    "    tokens = line.split('\\t')\n",
    "    if len(tokens) >= 3:\n",
    "        new_sentences.append(tokens[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'इन दिनों उसी जमीन पर काम चल रहा है।'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentences[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = new_sentences\n",
    "small = sentences[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_doc('hindi_small.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4448649"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.split('\\n')\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done\n",
      "10.0 % done\n",
      "20.0 % done\n",
      "30.0 % done\n",
      "40.0 % done\n",
      "50.0 % done\n",
      "60.0 % done\n",
      "70.0 % done\n",
      "80.0 % done\n",
      "90.0 % done\n"
     ]
    }
   ],
   "source": [
    "tokenized_hindi = getTokenized(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भारी हिमपात के समय यहां पर 2 फीट बर्फ की मोटी परत जम जाती है।'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small = x\n",
    "small[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('hindi_small.txt', \"wb\")\n",
    "for line in small:\n",
    "    string_for_output = (line + '\\n').encode('utf8', 'replace')\n",
    "    fw.write(string_for_output)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'किया'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = tokenized_hindi[1010][-1]\n",
    "word\n",
    "# word.replace('।', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done\n",
      "10.0 % done\n",
      "20.0 % done\n",
      "30.0 % done\n",
      "40.0 % done\n",
      "50.0 % done\n",
      "60.0 % done\n",
      "70.0 % done\n",
      "80.0 % done\n",
      "90.0 % done\n"
     ]
    }
   ],
   "source": [
    "vocabulary, word2id, id2word = getVocabulary(tokenized_hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_hindi = removeOOV(tokenized_hindi, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "बीड़ा\n",
      "केजरीवाल\n",
      "फ्रांस\n",
      "झरना\n",
      "नामों\n",
      "प्रियंका\n",
      "लौटाई\n",
      "घनघोर\n",
      "भावनाएं\n",
      "रेललाइन\n",
      "एमएसपी\n",
      "आभा\n",
      "उगाहने\n",
      "वार्नर\n",
      "शोभा\n",
      "प्रत्यर्पण\n",
      "पहुँचाना\n",
      "बालाओं\n",
      "ा\n",
      "ओम्\n",
      "मोहाली\n",
      "धारीगाढ\n",
      "थ्\n",
      "अल्\n",
      "लगानी\n",
      "प्रपत्र\n",
      "राजस्\n",
      "शुगर\n",
      "सांगठनिक\n",
      "कालू\n",
      "उन्\n",
      "बागडोर\n",
      "बट्टा\n",
      "ईश\n",
      "बाइबिल\n",
      "सुपारी\n",
      "अनाथ\n",
      "हफ्तेभर\n",
      "जहाज़\n",
      "पेशानी\n",
      "अधिष्ठापन\n",
      "मोज़िला\n",
      "ष्\n",
      "आत्मविश्वासी\n",
      "माइ\n",
      "मुताबित\n",
      "आयुवानसिंह\n",
      "पुनर्गठन\n",
      "कोठों\n",
      ".\n",
      "शुभ\n",
      "मुर्दा\n",
      "भूलता\n",
      "प्राथमिक\n",
      "तत्पश्चात\n",
      "कारवाई\n",
      "गाढ़ी\n",
      "कलंदर\n",
      "संवारा\n",
      "सिल्क\n",
      "चुभती\n",
      "तनेजा\n",
      "तत्कालिक\n",
      "क्रिस्टी\n",
      "कलाएँ\n",
      "स्टेट्स\n",
      "any\n",
      "भीड़\n",
      "एजेंडा\n",
      "कांग्रेसजनों\n",
      "डि\n",
      "जिलेभर\n",
      "करी\n",
      "सड़ा\n",
      "देश-काल\n",
      "आस्तिक\n",
      "मध्य\n",
      "गलियों\n",
      "बिल्डर\n",
      "मार्गशीर्ष\n",
      "पूर्वाह्न\n",
      "सपोर्ट\n",
      "ऊन\n",
      "देखते\n",
      "बेहतरीन\n",
      "जपते\n",
      "मढ़\n",
      "बजती\n",
      "बहकर\n",
      "उपलक्ष्य\n",
      "महिलायें\n",
      "पीएस\n",
      "भारतविरोधी\n",
      "प्रोफेशनल्स\n",
      "प्लान\n",
      "ख\n",
      "प्राइम\n",
      "गुंडा\n",
      "तस\n",
      "दूँगी\n",
      "कॅंपस\n",
      "मात्र\n",
      "यो\n",
      "झगड़ों\n",
      "सुहानी\n",
      "चारा\n",
      "कपडे\n",
      "आग्रह\n",
      "बुलाता\n",
      "भ्रष्\n",
      "दुर्गे\n",
      "हज़ारे\n",
      "पिछड़ों\n",
      "प्रवृति\n",
      "शिमला\n",
      "संवाददाता-\n",
      "देसाई\n",
      "झांसे\n",
      "सँवर\n",
      "बेवकूफ़\n",
      "डम्बलडोर\n",
      "लोकपाल\n",
      "कुत्तो\n",
      "जनार्दन\n",
      "चिट्ठाचर्चा\n",
      "बांध\n",
      "ज़रिये\n",
      "उमेश\n",
      "भेद-भाव\n",
      "बरन\n",
      "अकिंचन\n",
      "टेपिंग\n",
      "रॉयल्स\n",
      "रघुराज\n",
      "मित्र\n",
      "दरबारियों\n",
      "कहिन\n",
      "पांवों\n",
      "ऑप्शन\n",
      "रुबरु\n",
      "दिल्ली-\n",
      "वस्तुएं\n",
      "एक्स्प्रेस\n",
      "वाहनों\n",
      "शैलेश\n",
      "दुर्गेश\n",
      "नाराज़गी\n",
      "पढ़\n",
      "डालें\n",
      "रतन\n",
      "पहचानों\n",
      "दवाब\n",
      "मुहरबंद\n",
      "अर्थः\n",
      "भयावहता\n",
      "रायगढ़\n",
      "कुशवाह\n",
      "आखर\n",
      "कगार\n",
      "सवेरे\n",
      "फीड\n",
      "कुचक्र\n",
      "दरवेश\n",
      "परमात्मा\n",
      "साबला\n",
      "कोकिंग\n",
      "लेखकों\n",
      "जद्दोजहद\n",
      "किन्नौर\n",
      "शास्त्री\n",
      "नवागंतुकों\n",
      "लाख\n",
      "चिढ़\n",
      "नन\n",
      "रेस्तरां\n",
      "चौकीदार\n",
      "खुसरो\n",
      ".यह\n",
      "मुर्गी\n",
      "दिखावट\n",
      "सरीखे\n",
      "खींचने\n",
      "बैज\n",
      "बंटी\n",
      "कारवाँ\n",
      "जस्ता\n",
      "पर्ची\n",
      "शर्मिला\n",
      "सुहाती\n",
      "मर्ज\n",
      "गुलदस्ता\n",
      "प्रस्तूत\n",
      "हंसने\n",
      "करवटें\n",
      "महात्माओं\n",
      "मटके\n",
      "पता\n",
      "जलेगा\n",
      "सोहबत\n",
      "एडिट\n",
      "फूलो\n",
      "स्वीकारा\n",
      "विभ्रम\n",
      "बांधते\n",
      "एक्सपोर्ट\n",
      "आज्ञाओं\n",
      "ताम्बे\n",
      "मौकों\n",
      "उत्सुक\n",
      "विजिटिंग\n",
      "चैनलो\n",
      "फोरलेन\n",
      "करीब-करीब\n",
      "भारतदीप\n",
      "शीघ्रता\n",
      "पालते\n",
      "महापंचायत\n",
      "ठहाका\n",
      "सबल\n",
      "नज़ारा\n",
      "भारत\n",
      "डुग\n",
      "संयत\n",
      "वीडियो\n",
      "चलाये\n",
      "कालका\n",
      "शासक\n",
      "अधिकृत\n",
      "लीबिया\n",
      "बेसमेंट\n",
      "जगाता\n",
      "डिस्क\n",
      "छोड\n",
      "दुर्गापुर\n",
      "विज्ञानियों\n",
      "औपनिवेशिक\n",
      "रहिये\n",
      "post\n",
      "डाइ\n",
      "सत्यता\n",
      "अकुशल\n",
      "देवनारायण\n",
      "प्याला\n",
      "कोतवाली\n",
      "बारहवें\n",
      "================================================================================\n",
      "समर\n",
      "ओवल\n",
      "खनिज\n",
      "मुस्कराना\n",
      "हल्के-फुल्के\n",
      "चौखट\n",
      "व्यापी\n",
      "लालकृष्ण\n",
      "खा\n",
      "बताओ\n",
      "झुकती\n",
      "डू\n",
      "मज़ारों\n",
      "प्रशिक्षित\n",
      "समझाए\n",
      "महफूज़\n",
      "लिफाफा\n",
      "दैत्यराज\n",
      "कांगे्रस\n",
      "राजग\n",
      "लगेगा\n",
      "बोर\n",
      "सैंडविच\n",
      "परिवर्तित\n",
      "इंजीनियरी\n",
      "कद्दावर\n",
      "चलाईं\n",
      "हमदर्द\n",
      "विश्लेषिकी\n",
      "हताशा\n",
      "लेकिन\n",
      "बालकोनी\n",
      "टहलने\n",
      "कम्युनिटी\n",
      "दृष्टियों\n",
      "वरुणा\n",
      "ड्राइव\n",
      "अनभिज्ञ\n",
      "खोने\n",
      "नजरिया\n",
      "lsquo\n",
      "खस\n",
      "समझाता\n",
      "कतई\n",
      "तरफदारी\n",
      "गुजरेगी\n",
      "सुनो\n",
      "समस्या-\n",
      "पक्के\n",
      "शुभंकर\n",
      "किया|\n",
      "रुपी\n",
      "झुर्रियों\n",
      "भूतिया\n",
      "बेबो\n",
      "पियात्सा\n",
      "रैली\n",
      "मनोरंजन\n",
      "वॉकर\n",
      "रहन-सहन\n",
      "सुब्रमण्यम\n",
      "लगो\n",
      "याओं\n",
      "पार्श्र्वगायक\n",
      "ज्ञानार्जन\n",
      "जीना\n",
      "मुकाबला\n",
      "प्रश्नावली\n",
      "रीति\n",
      "पाईप\n",
      "न्यायाधिकरण\n",
      "महिषासुर\n",
      "सदाचार\n",
      "सेवक\n",
      "योनि\n",
      "जेलर\n",
      "उभरता\n",
      "pad\n",
      "फटकारा\n",
      "स्विट्जरलैंड\n",
      "बेटर\n",
      "कनिमोझी\n",
      "सूझा\n",
      "आवर्ती\n",
      "नायकों\n",
      "शेयरधारक\n",
      "अटलांटा\n",
      "समाचारों\n",
      "अगुआई\n",
      "गोवारीकर\n",
      "शांति\n",
      "चिकित्सकीय\n",
      "देना\n",
      "कार्बेट\n",
      "एलर्जी\n",
      "छन्द\n",
      "उपाए\n",
      "तुलसीदास\n",
      "ढूंढ़ने\n",
      "शॉपिंग\n",
      "मीटरों\n",
      "मंे\n",
      "राई\n",
      "बजह\n",
      "जिमेदारी\n",
      "हंसे\n",
      "ठप्पा\n",
      "रिपब्लिक\n",
      "लॉक\n",
      "निपटने\n",
      "अम्ल\n",
      "केशों\n",
      "धूर्त\n",
      "पठनीय\n",
      "ऐडसेंस\n",
      "पुंज\n",
      "अधिसूचित\n",
      "चली\n",
      "सर्राफ\n",
      "संभल\n",
      "सांकेतिक\n",
      "॥\n",
      "दिक्कतें\n",
      "मिर्च\n",
      "कृतज्ञ\n",
      "डा\n",
      "पुकारते\n",
      "प्लेटफार्म\n",
      "परामर्श\n",
      "नतमस्तक\n",
      "बतिया\n",
      "उत्सवों\n",
      "मिनटों\n",
      "स्तोत्र\n",
      "सीआईएसएफ\n",
      "बला\n",
      "अभियोग\n",
      "विधिक\n",
      "बर्मा\n",
      "हमशक्ल\n",
      "फिजा\n",
      "धर्मनिरपेक्षता\n",
      "लीटर\n",
      "हैपी\n",
      "सीसीआई\n",
      "छिप\n",
      "पत्तियां\n",
      "कंकर\n",
      "शिक्षक\n",
      "ऍम\n",
      "भरपाई\n",
      "सुपुर्द\n",
      "पूजन\n",
      "तोते\n",
      "फैन्स\n",
      "दुरूस्त\n",
      "लदा\n",
      "जर्नलिस्ट\n",
      "गाड़ी\n",
      "राजनगर\n",
      "गोलाकार\n",
      "सारस्वत\n",
      "जयेंद्र\n",
      "थामकर\n",
      "उद्धृत\n",
      "पराजय\n",
      "फैमिली\n",
      "ब्रेड\n",
      "टेक्निकल\n",
      "भिक्षु\n",
      "आडम्बर\n",
      "घुटता\n",
      "रचती\n",
      "जाए\n",
      "मीटिंग\n",
      "ग्रोवर\n",
      "चुंबकीय\n",
      "परलोक\n",
      "कंटीली\n",
      "गोस्वामी\n",
      "गानों\n",
      "कसाई\n",
      "all\n",
      "गिरिराज\n",
      "पढ़ाता\n",
      "चेतेश्वर\n",
      "शहनाई\n",
      "कबीरा\n",
      "तासीर\n",
      "चढ़ाई\n",
      "स्पाइस\n",
      "पहरा\n",
      "आशियाना\n",
      "नाज़\n",
      "ऑरेंज\n",
      "आईएसआई\n",
      "अकाउंट\n",
      "पोलित\n",
      "प्रसार\n",
      "योजनाबद्ध\n",
      "ल्यू\n",
      "बरसते\n",
      "इनबॉक्स\n",
      "बैठे\n",
      "मानवता\n",
      "►\n",
      "ग्रस्त\n",
      "ज़बर्दस्त\n",
      "रिकी\n",
      "बचपन\n",
      "मार्च\n",
      "नेत्रों\n",
      "जयवीर\n",
      "सन्ध्या\n",
      "हौंसले\n",
      "मिठास\n",
      "मुस्तफा\n",
      "अदम्य\n",
      "विंदू\n",
      "मारीशस\n",
      "भड़काऊ\n",
      "ज्यों-ज्यों\n",
      "हंसा\n",
      "फिजिक्स\n",
      "भूनिर्माण\n",
      "डलवा\n",
      "पे\n",
      "जीर्ण\n",
      "बंधुओ\n",
      "करूंगी\n",
      "रुझान\n",
      "ऋण\n",
      "जमींदार\n",
      "ग्रहीय\n",
      "चौगुना\n",
      "हैक\n",
      "श्रद्धांजलि\n",
      "आधी\n",
      "कृषकों\n",
      "झज्जर\n",
      "प्रचलन\n",
      "साप्ताहिक\n",
      "नय्यर\n",
      "मनपसंद\n",
      "विदित\n",
      "संयुक्तराष्ट्र\n",
      "गतिरोध\n",
      "पुलकित\n",
      "पड़ेगा\n",
      "even\n",
      "तेजसिंह\n",
      "पहाड़\n",
      "सूत\n",
      "Total\n",
      "पार\n",
      "संबंधों\n",
      "चीजे\n",
      "बनेगा\n",
      "ऑटिज़्म\n",
      "उपज\n",
      "जमाने\n",
      "शीट\n",
      "लिख्खूं\n",
      "बन्धन\n",
      "संकेतों\n",
      "टेरी\n",
      "कैनवास\n",
      "भ्रूण\n",
      "नानी\n",
      "पीकर\n",
      "भाटिया\n",
      "मातहत\n",
      "रागदरबारी\n",
      "लिखती\n",
      "कुचलते\n",
      "सुनाती\n",
      "उलटा\n",
      "सुपरहिट\n",
      "मिलाने\n",
      "धर्मयुग\n",
      "बनाइये\n",
      "हंसी\n",
      "रेवती\n",
      "जेसीबी\n",
      "गीली\n",
      "काँपने\n",
      "रूट\n",
      "वध\n",
      "उतरते\n",
      "जल्दी-जल्दी\n",
      "अत्यंत\n",
      "बढ़ाना\n",
      "एमएम\n",
      "बहुत-से\n",
      "भलीभाँति\n",
      "अंधे\n",
      "प्रोफेशनल\n",
      "दोहरे\n",
      "बीएसएफ\n",
      "वॉल्यूम\n",
      "सरणी\n",
      "आरजू\n",
      "शुभारंभ\n",
      "श्रीमति\n",
      "आग्रहों\n",
      "ऊँ\n",
      "सुना\n",
      "तालियों\n",
      "पुंछ\n",
      "ढांपने\n",
      "श्रेणियाँ\n",
      "अदालतों\n",
      "केनी\n",
      "दलितों\n",
      "जयंति\n",
      "ओनर\n",
      "वकील\n",
      "बीसियों\n",
      "स्वावलंबी\n",
      "बौरा\n",
      "उभरेगा\n",
      "देवत्व\n",
      "कान\n",
      "मोक्ष\n",
      "शायरों\n",
      "स्वंय\n",
      "ऐक्टर्स\n",
      "भूखा\n",
      "सुर्खी\n",
      "मुखड़ा\n",
      "मलबे\n",
      "अक्तूबर\n",
      "सौहार्द्र\n",
      "अर्ज़\n",
      "स्रोतों\n",
      "जाएँ\n",
      "एयरलाइनों\n",
      "महासेतु\n",
      "चीला\n",
      "इन्तजार\n",
      "आज\n",
      "वसल्लम\n",
      "गुरनाम\n",
      "खुलता\n",
      "नहाना\n",
      "दमा\n",
      "प्रायोगिक\n",
      "अब्दुल्लाह\n",
      "सुधरे\n",
      "सलीम\n",
      "पैसे\n",
      "पिलाई\n",
      "प्रवक्ता\n",
      "हत्यारे\n",
      "तहकीकात\n",
      "तव\n",
      "बिरंगे\n",
      "जयवर्द्धने\n",
      "भोजपुरी\n",
      "जेहादी\n",
      "शक्तिपीठ\n",
      "पलटे\n",
      "वाहिद\n",
      "बरामदगी\n",
      "सुनना\n",
      "सेविका\n",
      "अहं\n",
      "ग्राफिक\n",
      "दुनियां\n",
      "फूँक\n",
      "प्रमोट\n",
      "width=\n",
      "प्रषंसा\n",
      "दुलारा\n",
      "उन्नीस\n",
      "श्वेत\n",
      "सुल्तान\n",
      "पढ़ाते\n",
      "प्रतशित\n",
      "ज्वर\n",
      "लिफ्ट\n",
      "कामचलाऊ\n",
      "सिह\n",
      "बनायेंगे\n",
      "फ़ैली\n",
      "इज्जत\n",
      "काश्मीर\n",
      "खगोलीय\n",
      "माडलिंग\n",
      "Wednesday\n",
      "बनाने\n",
      "लोकायुक्त\n",
      "नायर\n",
      "निपटना\n",
      "सुनाई\n",
      "चढ\n",
      "अंगिका\n",
      "लाइसेंसिंग\n",
      "उत्प्रेरक\n",
      "मिटाते\n",
      "लोअर\n",
      "अवधी\n",
      "सकने\n",
      "विकीर्ण\n",
      "होकर\n",
      "जानते\n",
      "हिरन\n",
      "बंदरों\n",
      "दाताराम\n",
      "चिपक\n",
      "निकालें\n",
      "स्वीकार्यता\n",
      "वाणी\n",
      "फ़ोल्डर\n",
      "क्यारियों\n",
      "बरसों\n",
      "प्रशिक्षण\n",
      "सुरंग\n",
      "संधान\n",
      "विचरण\n",
      "ख़िदमत\n",
      "स्मरण\n",
      "पिंटू\n",
      "दबाएँ\n",
      "लुटाने\n",
      "शिप\n",
      "सिल्वर\n",
      "कारनामों\n",
      "थल\n",
      "बोलोगे\n",
      "आँख\n",
      "जदयू\n",
      "शिवराजसिंह\n",
      "कोक\n",
      "शांडिल्य\n",
      "गायों\n",
      "पोते\n",
      "उड़ाने\n",
      "क्रिस्टीन\n",
      "रुद्राक्ष\n",
      "छपीं\n",
      "वक़्त\n",
      "पापड़\n",
      "ठन\n",
      "तर्रार\n",
      "बगैर\n",
      "पीढि़यों\n",
      "जेबें\n",
      "खुला\n",
      "जलाएं\n",
      "बेसहारा\n",
      "स्टेटस\n",
      "स्टूडेंट\n",
      "उसीके\n",
      "मांझी\n",
      "अनजान\n",
      "लाईट\n",
      "दर्शाने\n",
      "लेटकर\n",
      "मूंदे\n",
      "दिक्कत\n",
      "पहना\n",
      "सुइट\n",
      "कुंडल\n",
      "सेठ\n",
      "उठाये\n",
      "मोज़ेक\n",
      "मखमली\n",
      "जंगली\n",
      "लंबी-लंबी\n",
      "जेनिफर\n",
      "ऑउंस\n",
      "लकवा\n",
      "कदमों\n",
      "टार्च\n",
      "धर्म\n",
      "विजयी\n",
      "मार्किट\n",
      "tha\n",
      "ऑप्टिकल\n",
      "रिपोर्\n",
      "शिल्पा\n",
      "बृज\n",
      "शर्मीले\n",
      "सर्वेक्षण\n",
      "स्वतन्त्रता\n",
      "कॉलोनियों\n",
      "जिप्सम\n",
      "सिलवा\n",
      "शिरडी\n",
      "आएँगी\n",
      "शेरशाह\n",
      "आकृष्ट\n",
      "पार्ट\n",
      "रॉय\n",
      "भोग-विलास\n",
      "दृष्टांत\n",
      "बल्ली\n",
      "In\n",
      "टा\n",
      "रासुका\n",
      "संपत्तियां\n",
      "साइमंड्स\n",
      "हीमोग्लोबिन\n",
      "चुग\n",
      "मानवमात्र\n",
      "इ\n",
      "शा\n",
      "सौंप\n",
      "अत्यन्त\n",
      "हॉपकिंस\n",
      "ख़तरा\n",
      "फांसी\n",
      "मोहि\n",
      "हार्वर्ड\n",
      "पिघलने\n",
      "ट्रिब्यून\n",
      "गोंद\n",
      "विघटन\n",
      "शुभचिंतकों\n",
      "डीआईजी\n",
      "आख़िर\n",
      "काफ़िर\n",
      "महत्त्वज्योतिष\n",
      "हमने\n",
      "सुषमा\n",
      "जानेवाली\n",
      "निरंकुश\n",
      "भाइयो\n",
      "शिवराम\n",
      "पिपली\n",
      "एव\n",
      "समायोजन\n",
      "प्रधानमंत्री\n",
      "बिजी\n",
      "ह्यूमर\n",
      "अवसरों\n",
      "रग्बी\n",
      "पिटारा\n",
      "सुसंस्कृत\n",
      "क्रिकेटरों\n",
      "बैंगन\n",
      "ब्रा\n",
      "संवादों\n",
      "आवेदक\n",
      "रूमी\n",
      "मुआवजे\n",
      "सिखाता\n",
      "इच्छाएं\n",
      "अऊर\n",
      "चंगुल\n",
      "याने\n",
      "शक्\n",
      "विश्वसनीय\n",
      "दाम\n",
      "बोलना\n",
      "सचेत\n",
      "उकता\n",
      "एकमात्र\n",
      "हिमायती\n",
      "धूम\n",
      "ब्लॉगर\n",
      "भगवान्\n",
      "क्वांटम\n",
      "ख्याल\n",
      "राज-काज\n",
      "विशेषतः\n",
      "स्नानार्थियों\n",
      "रूचि\n",
      "झेलनी\n",
      "प्रान्तीय\n",
      "फिजिकल\n",
      "जांच-पड़ताल\n",
      "गहने\n",
      "देगें\n",
      "वृत्ताकार\n",
      "क्वालीफायर\n",
      "तवे\n",
      "छप्पन\n",
      "रोमा\n",
      "विजेट\n",
      "गतिविधि\n",
      "शिविर\n",
      "छपने\n",
      "विपुल\n",
      "निकलेंगे\n",
      "ड्रायर\n",
      "प्राधिकारी\n",
      "पीढ़ी\n",
      "भांग\n",
      "टर्मिनल\n",
      "किसे\n",
      "जुड़\n",
      "संस्थापकों\n",
      "हजारे\n",
      "लोहिया\n",
      "मानता\n",
      "प्रतिबिंबित\n",
      "आबरू\n",
      "रजिस्टरों\n",
      "वाक\n",
      "भोजन\n",
      "स्वर्गदूतों\n",
      "अनाम\n",
      "विषाक्त\n",
      "वीर्य\n",
      "गोपनीय\n",
      "दिनांकित\n",
      "बाउ\n",
      "अपनत्व\n",
      "इनसान\n",
      "पस्त\n",
      "संत\n",
      "हस्तशिल्प\n",
      "ढलने\n",
      "बीतने\n",
      "संगठन\n",
      "दसों\n",
      "रामनगर\n",
      "टूटकर\n",
      "मुजाहिद्दीन\n",
      "ऋतिक\n",
      "पिस\n",
      "मटकी\n",
      "क़रीब\n",
      "सुलझ\n",
      "निति\n",
      "चूहे\n",
      "इंदिरापुरम\n",
      "लिख\n",
      "नस्लीय\n",
      "खायी\n",
      "लड़कपन\n",
      "अस्वाभाविक\n",
      "उठेगा\n",
      "पीढ़ियाँ\n",
      "गुह्य\n",
      "दिखे\n",
      "कीटनाशकों\n",
      "ड्राइवरों\n",
      "ललकार\n",
      "पहलुओं\n",
      "बनाई\n",
      "व्हिस्की\n",
      "कुंए\n",
      "जर्सी\n",
      "कवर\n",
      "आईडी\n",
      "एमएससी\n",
      "ओसामा\n",
      "खिलाड़ियों\n",
      "करनेवाला\n",
      "नवनिर्वाचित\n",
      "टायसन\n",
      "लाज\n",
      "शब्\n",
      "एफएमपी\n",
      "बजते\n",
      "निकास\n",
      "मंदोदरी\n",
      "हेनरी\n",
      "ढ़ंग\n",
      "धक\n",
      "अभ्यस्त\n",
      "डिनर\n",
      "गिनते\n",
      "अंतिम\n",
      "तल्खी\n",
      "खुलेंगे\n",
      "एकड़\n",
      "रंगीन\n",
      "अम्मां\n",
      "these\n",
      "समयावधि\n",
      "interview\n",
      "अगली\n",
      "अनिर्णय\n",
      "खुद\n",
      "नमः\n",
      "ٱل\n",
      "मजनूँ\n",
      "लाइकोपीन\n",
      "राज्\n",
      "योजनाओं\n",
      "इच्छुक\n",
      "New\n",
      "दुकानदारों\n",
      "कंट्री\n",
      "हालिया\n",
      "तरंगे\n",
      "अपघटन\n",
      "संप्रभुता\n",
      "झुकी\n",
      "कैरेबियाई\n",
      "जॉनी\n",
      "बालकृष्ण\n",
      "फ्रायड\n",
      "खुलेआम\n",
      "चिंताओं\n",
      "काग़ज़\n",
      "निलंबित\n",
      "ग़रीब\n",
      "खोलेगी\n",
      "बड़ों\n",
      "तोल\n",
      "दिग्गजों\n",
      "होम्योपैथी\n",
      "कालांतर\n",
      "फ़ालतू\n",
      "ज़मानत\n",
      "टकराकर\n",
      "हीन\n",
      "एसके\n",
      "नशा\n",
      "सपाट\n",
      "स्नातकोत्तर\n",
      "कोकिला\n",
      "गुजरात\n",
      "वीक\n",
      "नार्वे\n",
      "नोडल\n",
      "प्रभावशीलता\n",
      "करुणानिधि\n",
      "मीडियम\n",
      "फ़्रेम\n",
      "also\n",
      "घबराया\n",
      "कसर\n",
      "साइना\n",
      "रामकृपाल\n",
      "चुनौतियाँ\n",
      "बसों\n",
      "विट्ठलभाई\n",
      "अभिव्यक्ति\n",
      "दूधनाथ\n",
      "हलफ़नामे\n",
      "सुशीला\n",
      "बेचा\n",
      "टेलीविजन\n",
      "ध्यानचंद\n",
      "बुद्धिबल\n",
      "तपते\n",
      "मसूद\n",
      "बाबजूद\n",
      "रूपमती\n",
      "नक्श\n",
      "टू\n",
      "डिस्कवरी\n",
      "परिपेक्ष्य\n",
      "तेजेन्द्र\n",
      "⋅\n",
      "चिंगारी\n",
      "पढने\n",
      "सिन्धी\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000,3000):\n",
    "    print(id2word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39489"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_small = 0\n",
    "MAX_SENTENCE_LENGTH = 10\n",
    "l = []\n",
    "hindi_tokenized_maxsent_10 = []\n",
    "for line in tokenized_hindi:\n",
    "    length  = len(line)\n",
    "    if(length <= MAX_SENTENCE_LENGTH):\n",
    "        num_small += 1\n",
    "        hindi_tokenized_maxsent_10.append(line)\n",
    "num_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated = []\n",
    "for line in hindi_tokenized_maxsent_10:\n",
    "    c = ' '.join(line)\n",
    "    concatenated.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concatenated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-51f45bca0539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconcatenated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'concatenated' is not defined"
     ]
    }
   ],
   "source": [
    "concatenated[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('hindi_small_concat_maxsent_10.txt', \"wb\")\n",
    "for line in concatenated:\n",
    "    string_for_output = (line + '\\n').encode('utf8', 'replace')\n",
    "    fw.write(string_for_output)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = load_doc('hindi_small_concat_maxsent_10.txt').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_x = x[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "import os\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(small_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.text.Tokenizer at 0x7f51404eee48>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3773"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.texts_to_sequences(small_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 147, 5, 646, 1222, 1223]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 3774\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 9941\n"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "for line in small_x:\n",
    "\tencoded = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(encoded)):\n",
    "\t\tsequence = encoded[i-1:i+1]\n",
    "\t\tsequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# for i in range(1, len(encoded)):\n",
    "# \tsequence = encoded[i-1:i+1]\n",
    "# \tsequences.append(sequence)\n",
    "# print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Sequence Length: 2\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9941"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 50)             188700    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3774)              192474    \n",
      "=================================================================\n",
      "Total params: 401,374\n",
      "Trainable params: 401,374\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=max_length-1))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_X = X[:5000]\n",
    "small_y = y[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9941/9941 [==============================] - 9s 918us/step - loss: 2.5851 - acc: 0.3889\n",
      "Epoch 2/50\n",
      "9941/9941 [==============================] - 8s 797us/step - loss: 2.5690 - acc: 0.3883\n",
      "Epoch 3/50\n",
      "9941/9941 [==============================] - 8s 838us/step - loss: 2.5542 - acc: 0.3897\n",
      "Epoch 4/50\n",
      "9941/9941 [==============================] - 7s 727us/step - loss: 2.5395 - acc: 0.3894\n",
      "Epoch 5/50\n",
      "9941/9941 [==============================] - 7s 675us/step - loss: 2.5261 - acc: 0.3878\n",
      "Epoch 6/50\n",
      "9941/9941 [==============================] - 6s 619us/step - loss: 2.5131 - acc: 0.3866\n",
      "Epoch 7/50\n",
      "9941/9941 [==============================] - 6s 652us/step - loss: 2.5018 - acc: 0.3886\n",
      "Epoch 8/50\n",
      "9941/9941 [==============================] - 6s 650us/step - loss: 2.4897 - acc: 0.3864\n",
      "Epoch 9/50\n",
      "9941/9941 [==============================] - 8s 817us/step - loss: 2.4799 - acc: 0.3868\n",
      "Epoch 10/50\n",
      "9941/9941 [==============================] - 8s 813us/step - loss: 2.4699 - acc: 0.3858\n",
      "Epoch 11/50\n",
      "9941/9941 [==============================] - 9s 883us/step - loss: 2.4603 - acc: 0.3855\n",
      "Epoch 12/50\n",
      "9941/9941 [==============================] - 7s 753us/step - loss: 2.4525 - acc: 0.3882\n",
      "Epoch 13/50\n",
      "9941/9941 [==============================] - 7s 690us/step - loss: 2.4437 - acc: 0.3862\n",
      "Epoch 14/50\n",
      "9941/9941 [==============================] - 7s 697us/step - loss: 2.4362 - acc: 0.3874\n",
      "Epoch 15/50\n",
      "9941/9941 [==============================] - 7s 664us/step - loss: 2.4292 - acc: 0.3852\n",
      "Epoch 16/50\n",
      "9941/9941 [==============================] - 6s 633us/step - loss: 2.4221 - acc: 0.3866\n",
      "Epoch 17/50\n",
      "9941/9941 [==============================] - 7s 752us/step - loss: 2.4152 - acc: 0.3879\n",
      "Epoch 18/50\n",
      "9941/9941 [==============================] - 8s 831us/step - loss: 2.4085 - acc: 0.3829\n",
      "Epoch 19/50\n",
      "9941/9941 [==============================] - 9s 861us/step - loss: 2.4030 - acc: 0.3832\n",
      "Epoch 20/50\n",
      "9941/9941 [==============================] - 8s 801us/step - loss: 2.3973 - acc: 0.3861\n",
      "Epoch 21/50\n",
      "8672/9941 [=========================>....] - ETA: 0s - loss: 2.3852 - acc: 0.3888"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-7799ab78ce31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "model.fit(X, y, epochs=50, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, max_length, seed_text, n_words):\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# pre-pad sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tarr = model.predict(encoded, verbose=0)\n",
    "\t\tarr[0][1] = 0\n",
    "\t\tarr[0][3] = 0\n",
    "# \t\t\tarr = arr[2:]\n",
    "\t\tyhat = np.argmax(arr)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tprint(yhat)\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\treturn in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "25\n",
      "46\n",
      "9\n",
      "918\n",
      "2201\n",
      "9\n",
      "918\n",
      "2201\n",
      "9\n",
      "918\n",
      "2201\n",
      "9\n",
      "918\n",
      "2201\n",
      "9\n",
      "918\n",
      "2201\n",
      "9\n",
      "918\n",
      "तुम्हारा नहीं कर रहे हैं तारे गाते हैं तारे गाते हैं तारे गाते हैं तारे गाते हैं तारे गाते हैं तारे\n",
      "12\n",
      "25\n",
      "46\n",
      "9\n",
      " नहीं कर रहे हैं\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(model, tokenizer, max_length-1, 'तुम्हारा', 20))\n",
    "print(generate_seq(model, tokenizer, max_length-1, '', 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
