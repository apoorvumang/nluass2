{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            \n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# # load dataset\n",
    "# filename = 'parallel/deu.txt'\n",
    "# doc = load_doc(filename)\n",
    "# # split into english-german pairs\n",
    "# pairs = to_pairs(doc)\n",
    "# # clean sentences\n",
    "# cp = clean_pairs(pairs)\n",
    "# # save clean pairs to file\n",
    "# save_clean_data(cp, 'old.pkl')\n",
    "# # spot check\n",
    "\n",
    "filename = 'sub.en-hi.en'\n",
    "eng = load_doc(filename).strip().split('\\n')\n",
    "\n",
    "filename = 'sub.en-hi.hi'\n",
    "hin = load_doc(filename).strip().split('\\n')\n",
    "\n",
    "\n",
    "pairs = []\n",
    "\n",
    "# for i in range(len(eng)):\n",
    "#     pairs.append([eng[i], hin[i]])\n",
    "# split into english-german pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = pairs[500][1]\n",
    "# line = line.split()\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "eng2 = []\n",
    "for line in eng:\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    eng2.append(' '.join(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list()\n",
    "for i in range(len(eng2)):\n",
    "    pair = list()\n",
    "    pair.append(eng2[i])\n",
    "    pair.append(hin[i])\n",
    "    pairs.append(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_small(pairs, MAX_LEN=10, MAX_LEN2=20):\n",
    "    new_pairs = list()\n",
    "    for pair in pairs:\n",
    "        line = pair[0] #english line maxlength\n",
    "        if(len(line.strip().split(' ')) <= MAX_LEN and len(pair[1].strip().split(' ')) <= MAX_LEN2):\n",
    "            pair2 = list()\n",
    "            pair2.append(pair[0])\n",
    "            pair2.append(pair[1])\n",
    "            new_pairs.append(pair2)\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = get_only_small(pairs, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77360"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(cp)\n",
    "cp = array(cp)\n",
    "save_clean_data(cp, 'hindi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: hindi-both.pkl\n",
      "Saved: hindi-train.pkl\n",
      "Saved: hindi-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('hindi.pkl')\n",
    "shuffle(raw_dataset)\n",
    "# reduce dataset size\n",
    "n_sentences = 77360\n",
    "n_train = int(77360*0.9)\n",
    "dataset = raw_dataset[:n_sentences]\n",
    "# random shuffle\n",
    "\n",
    "# split into train/test\n",
    "train, test = dataset[:n_train], dataset[n_train:]\n",
    "# save\n",
    "save_clean_data(dataset, 'hindi-both.pkl')\n",
    "save_clean_data(train, 'hindi-train.pkl')\n",
    "save_clean_data(test, 'hindi-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('hindi-both.pkl')\n",
    "train = load_clean_sentences('hindi-train.pkl')\n",
    "test = load_clean_sentences('hindi-test.pkl')\n",
    "\n",
    "# dataset = load_clean_sentences('english-german-10-10-both.pkl')\n",
    "# train = load_clean_sentences('english-german-10-10-train.pkl')\n",
    "# test = load_clean_sentences('english-german-10-10-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not before you', 'तुम से पहले नहीं ...'], dtype='<U91')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 15567\n",
      "English Max Length: 10\n",
      "German Vocabulary Size: 21617\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['तुम से पहले नहीं ...', 'अन्य तरीके से.', '- उठो.', ..., 'डिक!',\n",
       "       'लेफ्टिनेंट', 'उह... .'], dtype='<U91')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 15567\n",
      "English Max Length: 10\n",
      "German Vocabulary Size: 21617\n",
      "German Max Length: 10\n",
      "Set eng_length to ger_length\n",
      "Encoding trainX\n",
      "Encoding trainY...\n",
      "Done\n",
      "Encoding testY...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('hindi-both.pkl')\n",
    "train = load_clean_sentences('hindi-train.pkl')\n",
    "test = load_clean_sentences('hindi-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "eng_length = ger_length\n",
    "print('Set eng_length to ger_length')\n",
    "# prepare training data\n",
    "print('Encoding trainX')\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "print('Encoding trainY...')\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "print('Done')\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "print('Encoding testY...')\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7736"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 256)           5533952   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 10, 15567)         4000719   \n",
      "=================================================================\n",
      "Total params: 10,585,295\n",
      "Trainable params: 10,585,295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define NMT model\n",
    "from keras.layers import RNN\n",
    "from keras.layers import Bidirectional\n",
    "from custom_recurrents import AttentionDecoder\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units, return_sequences=False))\n",
    "#     model.add(AttentionDecoder(256, 256))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model_30march_hindi.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69624 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "69568/69624 [============================>.] - ETA: 0s - loss: 2.9592Epoch 00001: val_loss improved from inf to 2.79714, saving model to model_30march_hindi.h5\n",
      "69624/69624 [==============================] - 212s 3ms/step - loss: 2.9593 - val_loss: 2.7971\n",
      "Epoch 2/5\n",
      "69568/69624 [============================>.] - ETA: 0s - loss: 2.6748Epoch 00002: val_loss improved from 2.79714 to 2.62365, saving model to model_30march_hindi.h5\n",
      "69624/69624 [==============================] - 207s 3ms/step - loss: 2.6746 - val_loss: 2.6236\n",
      "Epoch 3/5\n",
      " 5120/69624 [=>............................] - ETA: 2:58 - loss: 2.5259"
     ]
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=5, batch_size=64, validation_data=(testX[:1000], testY[:1000]), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "source = testX[0]\n",
    "from numpy import argmax\n",
    "\n",
    "# load model\n",
    "# model = load_model('model_27march_mymodel2.h5')\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tsource = source.reshape((1, source.shape[0]))\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please let me do it | uberlass mir das bitte | please me it\n",
      "i bought the car | ich habe das auto gekauft | i bought the book book\n",
      "have you ever known tom to lie | habt ihr je erlebt dass tom gelogen hat | have you ever to to to\n",
      "he spoke very well | er sprach sehr gut | he looked very well\n",
      "i had a nightmare | ich hatte einen alptraum | i was a mistake\n",
      "why dont you take a taxi | warum fahren sie nicht mit dem taxi | why dont you get a a taxi\n",
      "i want to participate | ich mochte mitmachen | i want to\n",
      "we wish you well tom | wir wunschen dir alles gute tom | we you you you tom tom\n",
      "i asked tom to close the door | ich bat tom die tur zuzumachen | i asked tom to the\n",
      "do you see me | seht ihr mich | do you love me\n",
      "tom sounded busy | tom klang beschaftigt | tom is busy\n",
      "lesson two is easy | lektion zwei ist einfach | your people is easy\n",
      "im an orphan | ich bin ein waisenkind | im a a\n",
      "lets go and see tom | lasst uns tom besuchen | lets go tom tom\n",
      "i am ready to die | ich bin zu sterben bereit | im ready ready\n",
      "the dog is smart | der hund ist schlau | the dog is beautiful\n",
      "he was in america at that time | er war gerade in amerika | he was in to to\n",
      "can i count on your help | kann ich auf ihre hilfe zahlen | can i teach your your\n",
      "tom began to perspire | tom fing an zu schwitzen | tom started to\n",
      "who wants hot chocolate | wer will heie schokolade | who wants to chocolate\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)-20, len(test)):\n",
    "    print(test[i][0],'|', test[i][1], '|', predict_sequence(model, eng_tokenizer, testX[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he said it was to a haircut'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Er sagte, es sei beabsichtigt, eine Kontamination zu vermeiden\"\n",
    "mine = encode_sequences(ger_tokenizer, ger_length, [text])\n",
    "predict_sequence(model, eng_tokenizer, mine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('ger_tokenizer', ger_tokenizer)\n",
    "np.save('eng_tokenizer', eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22,   3, 105, 588,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8,  196,   16,  305,   25,    1,  855,   34,  100, 5006,   15,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "#       source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "#       if i < 10:\n",
    "#           print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "        if i>1000:\n",
    "            break\n",
    "#   actual = [[['this', 'is', 'a', 'test']], [['this', 'is', 'a', 'test']]]\n",
    "#   predicted = [['this', 'is', 'a', 'test'], ['this', 'is', 'a', 'test']]\n",
    "    actual2 = []\n",
    "    for a in actual:\n",
    "        actual2.append([a])\n",
    "    actual = actual2\n",
    "\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.636426\n",
      "BLEU-2: 0.517624\n",
      "BLEU-3: 0.459045\n",
      "BLEU-4: 0.348639\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7788007830714049\n"
     ]
    }
   ],
   "source": [
    "# 1-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['meet', 'me', 'in', 'an', 'hour']]\n",
    "candidate = ['me', 'in' ,'an' ,'hour']\n",
    "score = sentence_bleu(reference, candidate,weights=(0.25, 0.25,0.25, 0.25))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
