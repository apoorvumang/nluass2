{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            \n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# # load dataset\n",
    "# filename = 'parallel/deu.txt'\n",
    "# doc = load_doc(filename)\n",
    "# # split into english-german pairs\n",
    "# pairs = to_pairs(doc)\n",
    "# # clean sentences\n",
    "# cp = clean_pairs(pairs)\n",
    "# # save clean pairs to file\n",
    "# save_clean_data(cp, 'old.pkl')\n",
    "# # spot check\n",
    "\n",
    "filename = 'sub.en-hi.en'\n",
    "eng = load_doc(filename).strip().split('\\n')\n",
    "\n",
    "filename = 'sub.en-hi.hi'\n",
    "hin = load_doc(filename).strip().split('\\n')\n",
    "\n",
    "\n",
    "pairs = []\n",
    "\n",
    "# for i in range(len(eng)):\n",
    "#     pairs.append([eng[i], hin[i]])\n",
    "# split into english-german pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = pairs[500][1]\n",
    "# line = line.split()\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "eng2 = []\n",
    "for line in eng:\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    # tokenize on white space\n",
    "    line = line.split()\n",
    "    # convert to lowercase\n",
    "    line = [word.lower() for word in line]\n",
    "    # remove punctuation from each token\n",
    "    line = [word.translate(table) for word in line]\n",
    "    # remove non-printable chars form each token\n",
    "    line = [re_print.sub('', w) for w in line]\n",
    "    # remove tokens with numbers in them\n",
    "    line = [word for word in line if word.isalpha()]\n",
    "    eng2.append(' '.join(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list()\n",
    "for i in range(len(eng2)):\n",
    "    pair = list()\n",
    "    pair.append(eng2[i])\n",
    "    pair.append(hin[i])\n",
    "    pairs.append(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_only_small(pairs, MAX_LEN=10, MAX_LEN2=20):\n",
    "    new_pairs = list()\n",
    "    for pair in pairs:\n",
    "        line = pair[0] #english line maxlength\n",
    "        if(len(line.strip().split(' ')) <= MAX_LEN and len(pair[1].strip().split(' ')) <= MAX_LEN2):\n",
    "            pair2 = list()\n",
    "            pair2.append(pair[0])\n",
    "            pair2.append(pair[1])\n",
    "            new_pairs.append(pair2)\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = get_only_small(pairs, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77360"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(cp)\n",
    "cp = array(cp)\n",
    "save_clean_data(cp, 'hindi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: hindi-both.pkl\n",
      "Saved: hindi-train.pkl\n",
      "Saved: hindi-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('hindi.pkl')\n",
    "shuffle(raw_dataset)\n",
    "# reduce dataset size\n",
    "n_sentences = 77360\n",
    "n_train = int(77360*0.9)\n",
    "dataset = raw_dataset[:n_sentences]\n",
    "# random shuffle\n",
    "\n",
    "# split into train/test\n",
    "train, test = dataset[:n_train], dataset[n_train:]\n",
    "# save\n",
    "save_clean_data(dataset, 'hindi-both.pkl')\n",
    "save_clean_data(train, 'hindi-train.pkl')\n",
    "save_clean_data(test, 'hindi-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('hindi-both.pkl')\n",
    "train = load_clean_sentences('hindi-train.pkl')\n",
    "test = load_clean_sentences('hindi-test.pkl')\n",
    "\n",
    "# dataset = load_clean_sentences('english-german-10-10-both.pkl')\n",
    "# train = load_clean_sentences('english-german-10-10-train.pkl')\n",
    "# test = load_clean_sentences('english-german-10-10-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer(num_words=1000)\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['hello vincent', '- हैलो विन्सेंट।'], dtype='<U91')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 15567\n",
      "English Max Length: 10\n",
      "German Vocabulary Size: 21617\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15566"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng_tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 15567\n",
      "English Max Length: 10\n",
      "German Vocabulary Size: 21617\n",
      "German Max Length: 10\n",
      "Set eng_length to ger_length\n",
      "Encoding trainX\n",
      "Encoding trainY...\n",
      "Done\n",
      "Encoding testY...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\"\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer(num_words=10000)\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "\treturn X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "\n",
    "# load datasets\n",
    "dataset = load_clean_sentences('hindi-both.pkl')\n",
    "train = load_clean_sentences('hindi-train.pkl')\n",
    "test = load_clean_sentences('hindi-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "eng_length = ger_length\n",
    "print('Set eng_length to ger_length')\n",
    "# prepare training data\n",
    "print('Encoding trainX')\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "print('Encoding trainY...')\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "print('Done')\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "print('Encoding testY...')\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "कानूनों\n",
      "सोना।\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in ger_tokenizer.word_index.items():\n",
    "    i += 1\n",
    "    print(key)\n",
    "    if i>1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define NMT model\n",
    "from keras.layers import RNN\n",
    "from keras.layers import Bidirectional\n",
    "from custom_recurrents import AttentionDecoder\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "# def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "#     model = Sequential()\n",
    "#     model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "# #     model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "#     model.add(LSTM(n_units, return_sequences=False))\n",
    "    \n",
    "# #     model.add(AttentionDecoder(256, 256))\n",
    "#     model.add(RepeatVector(tar_timesteps))\n",
    "#     model.add(LSTM(n_units, return_sequences=True))\n",
    "#     model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "#     model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "#     return model\n",
    "\n",
    "\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "    model.add(AttentionDecoder(128, tar_vocab, return_probabilities=True))\n",
    "#     model.add(RepeatVector(tar_timesteps))\n",
    "#     model.add(LSTM(n_units, return_sequences=True))\n",
    "#     model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    "\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 128)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# fit model\n",
    "filename = 'model_30march_hindi_attention.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69624 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "69600/69624 [============================>.] - ETA: 0s - loss: 1.9474Epoch 00001: val_loss improved from 2.24291 to 2.17702, saving model to model_30march_hindi_attention.h5\n",
      "69624/69624 [==============================] - 313s 4ms/step - loss: 1.9472 - val_loss: 2.1770\n",
      "Epoch 2/5\n",
      "69600/69624 [============================>.] - ETA: 0s - loss: 1.8141Epoch 00002: val_loss improved from 2.17702 to 2.13363, saving model to model_30march_hindi_attention.h5\n",
      "69624/69624 [==============================] - 165s 2ms/step - loss: 1.8141 - val_loss: 2.1336\n",
      "Epoch 3/5\n",
      "69600/69624 [============================>.] - ETA: 0s - loss: 1.6929Epoch 00003: val_loss improved from 2.13363 to 2.09312, saving model to model_30march_hindi_attention.h5\n",
      "69624/69624 [==============================] - 328s 5ms/step - loss: 1.6929 - val_loss: 2.0931\n",
      "Epoch 4/5\n",
      "69600/69624 [============================>.] - ETA: 0s - loss: 1.5844Epoch 00004: val_loss improved from 2.09312 to 2.06621, saving model to model_30march_hindi_attention.h5\n",
      "69624/69624 [==============================] - 305s 4ms/step - loss: 1.5845 - val_loss: 2.0662\n",
      "Epoch 5/5\n",
      "69600/69624 [============================>.] - ETA: 0s - loss: 1.4905Epoch 00005: val_loss improved from 2.06621 to 2.04963, saving model to model_30march_hindi_attention.h5\n",
      "69624/69624 [==============================] - 258s 4ms/step - loss: 1.4905 - val_loss: 2.0496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35126e1d68>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, epochs=5, batch_size=32, validation_data=(testX[:1000], testY[:1000]), callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "source = testX[0]\n",
    "from numpy import argmax\n",
    "\n",
    "# load model\n",
    "# model = load_model('model_27march_mymodel2.h5')\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tsource = source.reshape((1, source.shape[0]))\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bullshit | - बकवास! | bullshit\n",
      "imagine | कल्पना करो! | lmagine\n",
      "your dad used to say | तुम्हारे पिताजी कहा करते थे: | you you you you\n",
      "distract the alpha | ऐल्फ़ा का ध्यान बँटाओ। | the the the the the\n",
      "he found his son | -उसे उसका बेटा मिल गया! | he her her her\n",
      "no perfection anymore | अब और नहीं पूर्णता. | and and and the\n",
      "im right behind you | मैं ठीक तुम्हारे पीछे ही हूँ! | im you you\n",
      "my enemies are in the red keep | मेरे दुश्मन लाल रखें में हैं। | my my my my my\n",
      "and they harvest what they need | और वे वे क्या जरूरत फसल. | they they they they they\n",
      "he trusts you | उन्होंने कहा कि आप पर भरोसा करता है। | he he believe\n",
      "automated female voice | स्वचालित महिला की आवाज: | voice voice voice\n",
      "heres your food | यहाँ अपने भोजन है. | your your\n",
      "be my guest | मेरे मेहमान हो। | my my my\n",
      "and would i be sure that this | ♪ और अगर मुझे यकीन होता कि यह ♪ | if if if if if if again\n",
      "im sorry hes not here | मुझे खेद है, वह यहाँ नहीं हूँ. | im not not not\n",
      "im real sorry about your loss | मैं अपने नुकसान के बारे में खेद असली हूँ. | im about about about about\n",
      "whoohoo | व्हू - हू! | uh\n",
      "dick | डिक! | dick\n",
      "lieutenant | लेफ्टिनेंट | \n",
      "uh | उह... . | uh\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test)-20, len(test)):\n",
    "    print(test[i][0],'|', test[i][1], '|', predict_sequence(model, eng_tokenizer, testX[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he said it was to a haircut'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Er sagte, es sei beabsichtigt, eine Kontamination zu vermeiden\"\n",
    "mine = encode_sequences(ger_tokenizer, ger_length, [text])\n",
    "predict_sequence(model, eng_tokenizer, mine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('ger_tokenizer', ger_tokenizer)\n",
    "np.save('eng_tokenizer', eng_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22,   3, 105, 588,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8,  196,   16,  305,   25,    1,  855,   34,  100, 5006,   15,\n",
       "          0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorv/.local/lib/python3.5/site-packages/sklearn/externals/joblib/_multiprocessing_helpers.py:38: UserWarning: [Errno 12] Cannot allocate memory.  joblib will operate in serial mode\n",
      "  warnings.warn('%s.  joblib will operate in serial mode' % (e,))\n"
     ]
    }
   ],
   "source": [
    "# evaluate the skill of the model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "#       source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "#       if i < 10:\n",
    "#           print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "        if i>1000:\n",
    "            break\n",
    "#   actual = [[['this', 'is', 'a', 'test']], [['this', 'is', 'a', 'test']]]\n",
    "#   predicted = [['this', 'is', 'a', 'test'], ['this', 'is', 'a', 'test']]\n",
    "    actual2 = []\n",
    "    for a in actual:\n",
    "        actual2.append([a])\n",
    "    actual = actual2\n",
    "\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.275918\n",
      "BLEU-2: 0.115854\n",
      "BLEU-3: 0.062558\n",
      "BLEU-4: 0.015397\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7788007830714049\n"
     ]
    }
   ],
   "source": [
    "# 1-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['meet', 'me', 'in', 'an', 'hour']]\n",
    "candidate = ['me', 'in' ,'an' ,'hour']\n",
    "score = sentence_bleu(reference, candidate,weights=(0.25, 0.25,0.25, 0.25))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
