{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import nltk\n",
    "\n",
    "# threshold for minimum count to be considered a valid word\n",
    "MIN_VOCAB_COUNT = 8\n",
    "OOV_TOKEN = \"UNK\"\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='r', encoding='utf-8')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def RepresentsInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# remove numbers. they can be like 100 1090,200 2.123 etc\n",
    "# strategy is to remove punctuation and then check if its an integer\n",
    "def isNumber(word):\n",
    "    word_no_num = re.sub(r'[^\\w\\s]','',word)\n",
    "    if RepresentsInt(word_no_num):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#tokenizes raw strings\n",
    "def getTokenized(corpus):\n",
    "    lines = corpus.strip().split('~~')\n",
    "    lines = lines[:-1]\n",
    "    exclude = set(string.punctuation)\n",
    "    words_list = []  \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        words = nltk.word_tokenize(line)\n",
    "        words_nopunc_nonum = []\n",
    "        for word in words:\n",
    "            if word in exclude: # if punctuation\n",
    "                continue\n",
    "            else:\n",
    "                word = word.lower()\n",
    "                if(isNumber(word)): # if number\n",
    "                    word = \"###\"\n",
    "                words_nopunc_nonum.append(word)\n",
    "        words_list.append(words_nopunc_nonum)\n",
    "    return words_list\n",
    "\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "\n",
    "def getVocabulary(tokenized_corpus):\n",
    "    vocabulary = {}\n",
    "    for sentence in tokenized_corpus:\n",
    "        for token in sentence:\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = 1\n",
    "            else:\n",
    "                vocabulary[token] += 1\n",
    "    new_dict = {}\n",
    "    oov_count = 0 \n",
    "    # remove infrequent words\n",
    "    for word, count in vocabulary.items():\n",
    "        if(count >= MIN_VOCAB_COUNT):\n",
    "            new_dict[word] = count\n",
    "        else:\n",
    "            oov_count += count\n",
    "    new_dict[OOV_TOKEN] = oov_count\n",
    "    word2id = {w: idx for (idx, w) in enumerate(new_dict)}\n",
    "    id2word = {idx: w for (idx, w) in enumerate(new_dict)}\n",
    "    return new_dict, word2id, id2word\n",
    "\n",
    "def removeOOV(sentences, vocab):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        new_sent = []\n",
    "        for word in sentence:\n",
    "            if word in vocab:\n",
    "                new_sent.append(word)\n",
    "            else:\n",
    "                new_sent.append(OOV_TOKEN)\n",
    "        new_sentences.append(new_sent)\n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "# filename = 'data/deu.txt'\n",
    "# doc = load_doc(filename)\n",
    "# # split into english-german pairs\n",
    "# pairs = to_pairs(doc)\n",
    "# # clean sentences\n",
    "# clean_pairs = clean_pairs(pairs)\n",
    "# # save clean pairs to file\n",
    "# save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "# # spot check\n",
    "# for i in range(100):\n",
    "#     print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
    "\n",
    "eng = load_doc('data/news_2.en')\n",
    "ger = load_doc('data/news_2.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng = getTokenized(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ger = getTokenized(ger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word2id, id2word = getVocabulary(tokenized_ger)\n",
    "# tokenized_ger = removeOOV(tokenized_ger, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, word2id, id2word = getVocabulary(tokenized_eng)\n",
    "tokenized_eng = removeOOV(tokenized_eng, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216190 216190\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_ger), len(tokenized_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for i in range(len(tokenized_eng)):\n",
    "    pairs.append([tokenized_eng[i], tokenized_ger[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-nounk.pkl\n"
     ]
    }
   ],
   "source": [
    "pairs_new = []\n",
    "for i in range(len(tokenized_eng)):\n",
    "    if ('UNK' not in tokenized_eng[i]) and ('UNK' not in tokenized_ger[i]):\n",
    "        pairs_new.append([tokenized_eng[i], tokenized_ger[i]])\n",
    "save_clean_data(pairs_new, 'english-german-nounk.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102864"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = 0\n",
    "n_small = 0\n",
    "max_v = 0\n",
    "for key, value in vocabulary.items():\n",
    "    total_tokens += value\n",
    "    if(value > max_v):\n",
    "        max_v = value\n",
    "    if value < 3:\n",
    "        n_small += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297026\n"
     ]
    }
   ],
   "source": [
    "print(max_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
